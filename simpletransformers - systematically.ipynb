{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def read_file(fname: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads a filename and formats it properly for simpletransformers\"\"\"\n",
    "    df = pd.read_table(fname, sep=\"\\t\", header=None, names=\"text,labels,role\".split(\",\"))\n",
    "    offensive_ids = df.labels != \"Acceptable speech\"\n",
    "\n",
    "    df.labels[offensive_ids] = 1\n",
    "    df.labels[~offensive_ids] = 0\n",
    "    \n",
    "    df[\"labels\"] = df.labels.astype(np.int8)\n",
    "    df = df.drop(columns=[\"role\"])\n",
    "    return df\n",
    "\n",
    "def fine_tune_and_evaluate(\n",
    "    model_type,\n",
    "    model_name,\n",
    "    language\n",
    "                            ):\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    if lang not in {\"sl\", \"hr\", \"en\"}:\n",
    "        raise AttributeError(f\"Language {lang} is not valid\")\n",
    "    eval_file, train_file = f\"../data/lgbt-{lang}.test.tsv\" , f\"../data/lgbt-{lang}.train.tsv\"\n",
    "    train = read_file(train_file)\n",
    "    test = read_file(eval_file)\n",
    "    \n",
    "    from simpletransformers.classification import ClassificationModel\n",
    "    model_args = {\n",
    "        \"num_train_epochs\": 5,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"train_batch_size\": 40,\n",
    "        \"no_save\": True,\n",
    "    }\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        model_type, model_name, use_cuda=True,\n",
    "        args=model_args\n",
    "\n",
    "    )\n",
    "    model.no_save  = True\n",
    "    model.overwrite_output_dir = True\n",
    "    model.train_model(train, )\n",
    "\n",
    "    #print(model.eval_model(test))\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    y_true = test[\"labels\"]\n",
    "    y_pred = model.predict(list(test[\"text\"].values))[0]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.dense.bias', 'roberta.pooler.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:489: UserWarning: use_multiprocessing automatically disabled as camembert fails when using multiprocessing for feature conversion.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d807a4a0844400bd98d00ec8ee3b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7181d8f05ab14fe98c9d373b6f6c2fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py:941: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac05b7548254062a598f67111f2ce8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5050a6c64294a9cb9b1aed7bcacc178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8defb3ab0034318b4b8c3b5e24cb7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1945aa4166d940f18830bebeae7ccc97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a72c66bebeb4d1a87a5c180b87883cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6accc18687f481e9362e798463fc169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type camembert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at EMBEDDIA/sloberta were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/sloberta and are newly initialized: ['classifier.dense.bias', 'roberta.pooler.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768b93d9f2b94f33a269902aea0b37d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2844.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b96e77dcc634aa29390aee05f5a2f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc236b3cabd7450195d1353a881bca77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterr/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41ca66918824741a91536f375bdec74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508683b73b8f466c92028d7e6470933c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e78ef535004d91bcebe3bc0814d847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0854785b5074e738fff45a449f13022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=72.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654f8269b4c24cb2ae4ed56250b7b59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=900.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0d5571b8c44455b56d64b1f4097e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a692c6b9230045f580399e6cd7c7d510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8766a52558f64bfc93a0d69706a3035d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf3c1edf33a4458a73d9cbec4d7b5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5951b89443564d22b6b77a174c36444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3260599c82bb4cfbb84298d98688c08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617c940adbae423d8eeaf1c6a0948132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a69d29ba85e4d86bb1ae820f1d97669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9bfc524e4b4013a6d44ee2cb7584c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1142.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf231bafb314ed99eb1ca0d734a7a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=143.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "|classla/bcms-bertic|electra|hr|0.849|0.832|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ea9e938b08474f8944859c8b102d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9183db95ce894d54ada3faca4accdb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b646d62d171f4af182f4d143ef04e5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d3e9d00bdf47aca9bfe02c759fc44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f712a453937f430693af45d8ae4463e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef3a14342474f718c065788a9290a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc5c93df5654550859a9c215ab98aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=113.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbc89b7838d47c6bcc03962687e0a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1142.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214f941c81c349e5ae786a6ddd50f94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=143.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "|classla/bcms-bertic|electra|hr|0.849|0.832|\n",
      "|EMBEDDIA/crosloengual-bert|bert|hr|0.844|0.829|\n",
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "|classla/bcms-bertic|electra|hr|0.849|0.832|\n",
      "|EMBEDDIA/crosloengual-bert|bert|hr|0.844|0.829|\n",
      "|xlm-roberta-base|xlm-roberta|en|Error|'xlm-roberta'|\n",
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "|classla/bcms-bertic|electra|hr|0.849|0.832|\n",
      "|EMBEDDIA/crosloengual-bert|bert|hr|0.844|0.829|\n",
      "|xlm-roberta-base|xlm-roberta|en|Error|'xlm-roberta'|\n",
      "|xlm-roberta-large|xlm-roberta|en|Error|'xlm-roberta'|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be62ff4adbb844038facb65ec967cc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4819.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55492ee563074b97a955cc916d0cf2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab0875a98684f77bd6421d03d8d0c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 5', max=121.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30536fcb0c8b45fa8230031b35141b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 5', max=121.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5e0eb0293b4815938e2be93ba52cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 5', max=121.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95d25bf9d4e4015be74af792d559943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 5', max=121.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc807f2a0d734fd4a1e31fe8b066c060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 5', max=121.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5de07fd4194e6b9a5b25c9c0021f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1017.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2a9502a3534d529e60915008a72dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=128.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|model name| model type | language | accuracy | macro F1|\n",
      "|---|---|---|---|---|\n",
      "|xlm-roberta-base|xlm-roberta|sl|Error|'xlm-roberta'|\n",
      "|EMBEDDIA/sloberta|camembert|sl|0.730|0.729|\n",
      "|EMBEDDIA/sloberta|roberta|sl|Error|Can't load tokenizer for 'EMBEDDIA/sloberta'. Make sure that:\n",
      "\n",
      "- 'EMBEDDIA/sloberta' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or 'EMBEDDIA/sloberta' is the correct path to a directory containing relevant tokenizer files\n",
      "\n",
      "|\n",
      "|EMBEDDIA/crosloengual-bert|bert|sl|0.687|0.686|\n",
      "|xlm-roberta-base|xlm-roberta|hr|Error|'xlm-roberta'|\n",
      "|classla/bcms-bertic|electra|hr|0.849|0.832|\n",
      "|EMBEDDIA/crosloengual-bert|bert|hr|0.844|0.829|\n",
      "|xlm-roberta-base|xlm-roberta|en|Error|'xlm-roberta'|\n",
      "|xlm-roberta-large|xlm-roberta|en|Error|'xlm-roberta'|\n",
      "|roberta-base|roberta|en|0.850|0.802|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"sl\": [\n",
    "        (\"IMSyPP/hate_speech_slo\", \"bert\"),\n",
    "        (\"xlm-roberta-base\", \"xlm-roberta\"),\n",
    "        (\"EMBEDDIA/sloberta\", \"camembert\"),\n",
    "        (\"EMBEDDIA/sloberta\", \"roberta\"),\n",
    "        (\"EMBEDDIA/crosloengual-bert\", \"bert\"),\n",
    "        ],\n",
    "    \"hr\": [\n",
    "        (\"xlm-roberta-base\", \"xlm-roberta\"),\n",
    "        (\"classla/bcms-bertic\", \"electra\"),\n",
    "        (\"EMBEDDIA/crosloengual-bert\", \"bert\"),\n",
    "    ],\n",
    "    \"en\": [\n",
    "        (\"xlm-roberta-base\", \"xlm-roberta\"),\n",
    "        (\"xlm-roberta-large\", \"xlm-roberta\"),\n",
    "        (\"roberta-base\", \"roberta\"),\n",
    "    ]\n",
    "}\n",
    "output = \"\"\n",
    "output += \"|model name| model type | language | accuracy | macro F1|\\n\"\n",
    "output += \"|---|---|---|---|---|\\n\"\n",
    "for lang, confs in config_dict.items():\n",
    "    for conf in confs:\n",
    "        print(output)\n",
    "        model_name, model_type = conf\n",
    "        try:\n",
    "            a, f = fine_tune_and_evaluate(model_type, model_name, lang)\n",
    "            output += f\"|{model_name}|{model_type}|{lang}|{a:0.3f}|{f:0.3f}|\\n\"\n",
    "        except Exception as e:\n",
    "            #raise e\n",
    "            output += f\"|{model_name}|{model_type}|{lang}|Error|{e}|\\n\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
